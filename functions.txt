1) Change pose
- input: pointcloud of object in gripper
- output: grasp pose (3 or 6 DOF)
- feedback: grasp success (manual, force feedback, pressure feedback)

2) Adjust grasp
- input: joint values, biotac values, force feedback, pressure feedback(, hand torques)
- output: 3-5 PCA coefficients
- feedback: grasping: joint closure/pressure change/hand torques, grasped: grasp success (force feedback)

Optuna: try to optimize before reinforcement learning
Imitation: First, let the reinforcement agent learn the hardcoded values offline, to get a good starting policy -> change to different reward to encourage further exploration and improvements online
