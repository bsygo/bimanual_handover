For current approach, use a replay buffer to learn off-policy and utilize each sample multiple times
-> may be problematic with changing states such as different can positions

Trajectory learning:
- learn a trajectory and evaluate this instead of state by state pca changes
- find a way to extract information from the pointcloud to give to the network and generate the trajectory as an action
- trajectory can be defined by 6 values: 3 values for initial state (maybe optional) and 3 values for closing motion
- closing trajectory gets generated by adding 3 values constantly on points until finished
- trajectory finished until goal accomplished or error state reached (for example slipping fingers)
- then trajectory tested and labeled as successful or failure
- trajectories can then be learned using supervised learning instead of reinforcement learning

Future work:
- make second arm compliant as well, by reading pressure sensors and act accordingly
